{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "# TODO: allow for loading all three jet types together\n",
    "\n",
    "\n",
    "class JetNet(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch ``torch.utils.data.Dataset`` class for the JetNet dataset.\n",
    "\n",
    "    Features, in order: ``[eta, phi, pt, mask]``.\n",
    "\n",
    "    Will produce an iteratable of either the dataset alone, of shape\n",
    "    ``[num_jets, num_particles, num_features]``, or a tuple of the dataset and jet-level features\n",
    "    of each jet. Currently only the number of (non-zero-padded) particles per jet is available as\n",
    "    a jet feature.\n",
    "\n",
    "    If pt or hdf5 files are not found in the ``data_dir`` directory then:\n",
    "    If ``num_particles <= 30``, JetNet is downloaded from https://zenodo.org/record/6302454;\n",
    "    Else, JetNet150 is downloaded from https://zenodo.org/record/6302240\n",
    "\n",
    "    Args:\n",
    "        jet_type (str): 'g' (gluon), 't' (top quarks), or 'q' (light quarks).\n",
    "        data_dir (str): directory which contains (or in which to download) dataset.\n",
    "          Defaults to \"./\" i.e. the working directory.\n",
    "        download (bool): download the dataset, even if the hdf5 file exists already.\n",
    "          Defaults to False.\n",
    "        num_particles (int): number of particles to use, has to be less than or equal to 150.\n",
    "          Defaults to 30.\n",
    "        normalize (bool): normalize features for training or not, using parameters defined below.\n",
    "          Defaults to True.\n",
    "        feature_norms (Union[float, List[float]]): max value to scale each feature to.\n",
    "          Can either be a single float for all features, or a list of length ``num_features``.\n",
    "          Defaults to 1.0.\n",
    "        feature_shifts (Union[float, List[float]]): after scaling, value to shift feature by.\n",
    "          Can either be a single float for all features, or a list of length ``num_features``.\n",
    "          Defaults to 0.0.\n",
    "        use_mask (bool): Defaults to True.\n",
    "        train (bool): whether for training or testing. Defaults to True.\n",
    "        train_fraction (float): fraction of data to use as training - rest is for testing.\n",
    "          Defaults to 0.7.\n",
    "        num_pad_particles (int): how many out of ``num_particles`` should be zero-padded.\n",
    "          Defaults to 0.\n",
    "        use_num_particles_jet_feature (bool): Store the # of particles in each jet as a\n",
    "          jet-level feature. *Only works if using mask* i.e. if ``use_mask=True``. Defaults to True.\n",
    "        noise_padding (bool): instead of 0s, pad extra particles with Gaussian noise.\n",
    "          Only works if using mask. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    _num_non_mask_features = 3\n",
    "\n",
    "    # normalization used for ParticleNet training\n",
    "    _fpnd_feature_maxes = [1.6211985349655151, 0.520724892616272, 0.8934717178344727, 1.0]\n",
    "    _fpnd_feature_norms = 1.0\n",
    "    _fpnd_feature_shifts = [0.0, 0.0, -0.5, 0.0]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        jet_type: str,\n",
    "        data_dir: str = \"./\",\n",
    "        download: bool = False,\n",
    "        num_particles: int = 30,\n",
    "        normalize: bool = True,\n",
    "        feature_norms: List[float] = [1.0, 1.0, 1.0, 1.0],\n",
    "        feature_shifts: List[float] = [0.0, 0.0, -0.5, -0.5],\n",
    "        use_mask: bool = True,\n",
    "        train: bool = True,\n",
    "        train_fraction: float = 0.7,\n",
    "        num_pad_particles: int = 0,\n",
    "        use_num_particles_jet_feature: bool = True,\n",
    "        noise_padding: bool = False,\n",
    "    ):\n",
    "        assert jet_type in [\"top\", \"qcd\"], \"Invalid jet type\"\n",
    "\n",
    "        self.feature_norms = feature_norms\n",
    "        self.feature_shifts = feature_shifts\n",
    "        self.use_mask = use_mask\n",
    "        # in the future there'll be more jet features such as jet pT and eta\n",
    "        self.use_jet_features = use_num_particles_jet_feature and self.use_mask\n",
    "        self.noise_padding = noise_padding and self.use_masks\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Use JetNet150 if ``num_particles`` > 30\n",
    "        #use_150 = num_particles > 30\n",
    "        pt_file = f\"{data_dir}/{jet_type}{'150'}.pt\"\n",
    "\n",
    "        if not exists(pt_file) or download:\n",
    "            self.download_and_convert_to_pt(data_dir, jet_type)\n",
    "\n",
    "        logging.info(\"Loading dataset\")\n",
    "        dataset = self.load_dataset(pt_file, num_particles, num_pad_particles, use_mask)\n",
    "        self.num_particles = num_particles if num_particles > 0 else dataset.shape[1]\n",
    "\n",
    "        if self.use_jet_features:\n",
    "            jet_features = self.get_jet_features(dataset, use_num_particles_jet_feature)\n",
    "\n",
    "        logging.info(f\"Loaded dataset {dataset.shape = }\")\n",
    "        if normalize:\n",
    "            logging.info(\"Normalizing features\")\n",
    "            self.feature_maxes = self.normalize_features(dataset, feature_norms, feature_shifts)\n",
    "\n",
    "        if self.noise_padding:\n",
    "            dataset = self.add_noise_padding(dataset)\n",
    "\n",
    "        tcut = int(len(dataset) * train_fraction)\n",
    "\n",
    "        self.data = dataset[:tcut] if train else dataset[tcut:]\n",
    "        if self.use_jet_features:\n",
    "            self.jet_features = jet_features[:tcut] if train else jet_features[tcut:]\n",
    "\n",
    "        logging.info(\"Dataset processed\")\n",
    "\n",
    "    def download_and_convert_to_pt(self, data_dir: str, jet_type: str):\n",
    "        \"\"\"\n",
    "        Download jet dataset and convert and save to pytorch tensor.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): directory in which to save file.\n",
    "            jet_type (str): jet type to download, out of ``['g', 't', 'q']``.\n",
    "            use_150 (bool): download JetNet150 or JetNet. Defaults to False.\n",
    "\n",
    "        \"\"\"\n",
    "        import os\n",
    "\n",
    "        os.system(f\"mkdir -p {data_dir}\")\n",
    "        hdf5_file = f\"{data_dir}/{jet_type}{'150'}.hdf5\"\n",
    "\n",
    "        if not exists(hdf5_file):\n",
    "            logging.info(f\"Downloading {jet_type} jets hdf5\")\n",
    "            self.download(jet_type, hdf5_file)\n",
    "\n",
    "        logging.info(f\"Converting {jet_type} jets hdf5 to pt\")\n",
    "        self.hdf5_to_pt(data_dir, jet_type, hdf5_file)\n",
    "\n",
    "    def download(self, jet_type: str, hdf5_file: str):\n",
    "        \"\"\"\n",
    "        Downloads the ``jet_type`` jet hdf5 from Zenodo and saves it as ``hdf5_file``.\n",
    "\n",
    "        Args:\n",
    "            jet_type (str): jet type to download, out of ``['g', 't', 'q']``.\n",
    "            hdf5_file (str): path to save hdf5 file.\n",
    "            use_150 (bool): download JetNet150 or JetNet. Defaults to False.\n",
    "\n",
    "        \"\"\"\n",
    "        import requests\n",
    "        import sys\n",
    "\n",
    "        record_id = 6302240 \n",
    "        records_url = f\"https://zenodo.org/api/records/{record_id}\"\n",
    "        r = requests.get(records_url).json()\n",
    "        key = f\"{jet_type}{'150' }.hdf5\"\n",
    "\n",
    "        # finding the url for the particular jet type dataset\n",
    "        file_url = next(item for item in r[\"files\"] if item[\"key\"] == key)[\"links\"][\"self\"]\n",
    "        logging.info(f\"{file_url = }\")\n",
    "\n",
    "        # modified from https://sumit-ghosh.com/articles/python-download-progress-bar/\n",
    "        with open(hdf5_file, \"wb\") as f:\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            total = response.headers.get(\"content-length\")\n",
    "\n",
    "            if total is None:\n",
    "                f.write(response.content)\n",
    "            else:\n",
    "                downloaded = 0\n",
    "                total = int(total)\n",
    "\n",
    "                print(\"Downloading dataset\")\n",
    "                for data in response.iter_content(chunk_size=max(int(total / 1000), 1024 * 1024)):\n",
    "                    downloaded += len(data)\n",
    "                    f.write(data)\n",
    "                    done = int(50 * downloaded / total)\n",
    "                    sys.stdout.write(\n",
    "                        \"\\r[{}{}] {:.0f}%\".format(\n",
    "                            \"â–ˆ\" * done, \".\" * (50 - done), float(downloaded / total) * 100\n",
    "                        )\n",
    "                    )\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "        sys.stdout.write(\"\\n\")\n",
    "\n",
    "    def hdf5_to_pt(self, data_dir: str, jet_type: str, hdf5_file: str):\n",
    "        \"\"\"\n",
    "        Converts and saves downloaded hdf5 file to pytorch tensor.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): directory in which to save file.\n",
    "            jet_type (str): jet type to download, out of ``['g', 't', 'q']``.\n",
    "            hdf5_file (str): path to hdf5 file.\n",
    "            use_150 (bool): download JetNet150 or JetNet. Defaults to False.\n",
    "\n",
    "        \"\"\"\n",
    "        import h5py\n",
    "\n",
    "        pt_file = f\"{data_dir}/{jet_type}{'150' }.pt\"\n",
    "\n",
    "        with h5py.File(hdf5_file, \"r\") as f:\n",
    "            torch.save(Tensor(np.array(f[\"particle_features\"])), pt_file)\n",
    "\n",
    "    def load_dataset(\n",
    "        self, pt_file: str, num_particles: int, num_pad_particles: int = 0, use_mask: bool = True\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Load the dataset, optionally padding the particles.\n",
    "\n",
    "        Args:\n",
    "            pt_file (str): path to dataset .pt file.\n",
    "            num_particles (int): number of particles per jet to load\n",
    "              (has to be less than the number per jet in the dataset).\n",
    "            num_pad_particles (int): out of ``num_particles`` how many are to be zero-padded.\n",
    "              Defaults to 0.\n",
    "            use_mask (bool): keep or remove the mask feature. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: dataset tensor of shape ``[num_jets, num_particles, num_features]``.\n",
    "\n",
    "        \"\"\"\n",
    "        dataset = torch.load(pt_file).float()\n",
    "\n",
    "        # only retain up to ``num_particles``,\n",
    "        # subtracting ``num_pad_particles`` since they will be padded below\n",
    "        if 0 < num_particles - num_pad_particles < dataset.shape[1]:\n",
    "            dataset = dataset[:, : num_particles - num_pad_particles, :]\n",
    "\n",
    "        # pad with ``num_pad_particles`` particles\n",
    "        if num_pad_particles > 0:\n",
    "            dataset = torch.nn.functional.pad(dataset, (0, 0, 0, num_pad_particles), \"constant\", 0)\n",
    "\n",
    "        if not use_mask:\n",
    "            # remove mask feature from dataset if not needed\n",
    "            dataset = dataset[:, :, : self._num_non_mask_features]\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_jet_features(self, dataset: Tensor, use_num_particles_jet_feature: bool) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns jet-level features. `Will be expanded to include jet pT and eta.`\n",
    "\n",
    "        Args:\n",
    "            dataset (Tensor):  dataset tensor of shape [N, num_particles, num_features],\n",
    "              where the last feature is the mask.\n",
    "            use_num_particles_jet_feature (bool): `Currently does nothing,\n",
    "              in the future such bools will specify which jet features to use`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: jet features tensor of shape [N, num_jet_features].\n",
    "\n",
    "        \"\"\"\n",
    "        jet_num_particles = (torch.sum(dataset[:, :, -1], dim=1) / self.num_particles).unsqueeze(1)\n",
    "        logging.debug(\"{num_particles = }\")\n",
    "        return jet_num_particles\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_features(\n",
    "        self,\n",
    "        dataset: Tensor,\n",
    "        feature_norms: Union[float, List[float]] = 1.0,\n",
    "        feature_shifts: Union[float, List[float]] = 0.0,\n",
    "        fpnd: bool = False,\n",
    "    ) -> Optional[List]:\n",
    "        \"\"\"\n",
    "        Normalizes dataset features (in place),\n",
    "        by scaling to ``feature_norms`` maximum and shifting by ``feature_shifts``.\n",
    "\n",
    "        If the value in the List for a feature is None, it won't be scaled or shifted.\n",
    "\n",
    "        If ``fpnd`` is True, will normalize instead to the same scale as was used for the\n",
    "        ParticleNet training in https://arxiv.org/abs/2106.11535.\n",
    "\n",
    "        Args:\n",
    "            dataset (Tensor): dataset tensor of shape [N, num_particles, num_features].\n",
    "            feature_norms (Union[float, List[float]]): max value to scale each feature to.\n",
    "              Can either be a single float for all features, or a list of length ``num_features``.\n",
    "              Defaults to 1.0.\n",
    "            feature_shifts (Union[float, List[float]]): after scaling, value to shift feature by.\n",
    "              Can either be a single float for all features, or a list of length ``num_features``.\n",
    "              Defaults to 0.0.\n",
    "            fpnd (bool): Normalize features for ParticleNet inference for the\n",
    "              Frechet ParticleNet Distance metric. Will override `feature_norms`` and\n",
    "              ``feature_shifts`` inputs. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Optional[List]: if ``fpnd`` is False, returns list of length ``num_features``\n",
    "            of max absolute values for each feature. Used for unnormalizing features.\n",
    "\n",
    "        \"\"\"\n",
    "        num_features = dataset.shape[2]\n",
    "\n",
    "        if not fpnd:\n",
    "            feature_maxes = [\n",
    "                float(torch.max(torch.abs(dataset[:, :, i]))) for i in range(num_features)\n",
    "            ]\n",
    "        else:\n",
    "            feature_maxes = JetNet._fpnd_feature_maxes\n",
    "            feature_norms = JetNet._fpnd_feature_norms\n",
    "            feature_shifts = JetNet._fpnd_feature_shifts\n",
    "\n",
    "        if isinstance(feature_norms, float):\n",
    "            feature_norms = np.full(num_features, feature_norms)\n",
    "\n",
    "        if isinstance(feature_shifts, float):\n",
    "            feature_shifts = np.full(num_features, feature_shifts)\n",
    "\n",
    "        logging.debug(f\"{feature_maxes = }\")\n",
    "\n",
    "        for i in range(num_features):\n",
    "            if feature_norms[i] is not None:\n",
    "                dataset[:, :, i] /= feature_maxes[i]\n",
    "                dataset[:, :, i] *= feature_norms[i]\n",
    "\n",
    "            if feature_shifts[i] is not None and feature_shifts[i] != 0:\n",
    "                dataset[:, :, i] += feature_shifts[i]\n",
    "\n",
    "        if not fpnd:\n",
    "            return feature_maxes\n",
    "\n",
    "    def unnormalize_features(\n",
    "        self,\n",
    "        dataset: Union[Tensor, np.ndarray],\n",
    "        ret_mask_separate: bool = True,\n",
    "        is_real_data: bool = False,\n",
    "        zero_mask_particles: bool = True,\n",
    "        zero_neg_pt: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inverts the ``normalize_features()`` function on the input ``dataset`` array or tensor,\n",
    "        plus optionally zero's the masked particles and negative pTs.\n",
    "        Only applicable if dataset was normalized first\n",
    "        i.e. ``normalize`` arg into JetNet instance is True.\n",
    "\n",
    "        Args:\n",
    "            dataset (Union[Tensor, np.ndarray]): Dataset to unnormalize.\n",
    "            ret_mask_separate (bool): Return the jet and mask separately. Defaults to True.\n",
    "            is_real_data (bool): Real or generated data. Defaults to False.\n",
    "            zero_mask_particles (bool): Set features of zero-masked particles to 0.\n",
    "              Not needed for real data. Defaults to True.\n",
    "            zero_neg_pt (bool): Set pT to 0 for particles with negative pt.\n",
    "              Not needed for real data. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            Unnormalized dataset of same type as input. Either a tensor/array of shape\n",
    "            ``[num_jets, num_particles, num_features (including mask)]`` if ``ret_mask_separate``\n",
    "            is False, else a tuple with a tensor/array of shape\n",
    "            ``[num_jets, num_particles, num_features (excluding mask)]`` and another binary mask\n",
    "            tensor/array of shape ``[num_jets, num_particles, 1]``.\n",
    "        \"\"\"\n",
    "        if not self.normalize:\n",
    "            raise RuntimeError(\"Can't unnormalize features if dataset has not been normalized.\")\n",
    "\n",
    "        num_features = dataset.shape[2]\n",
    "\n",
    "        for i in range(num_features):\n",
    "            if self.feature_shifts[i] is not None and self.feature_shifts[i] != 0:\n",
    "                dataset[:, :, i] -= self.feature_shifts[i]\n",
    "\n",
    "            if self.feature_norms[i] is not None:\n",
    "                dataset[:, :, i] /= self.feature_norms[i]\n",
    "                dataset[:, :, i] *= self.feature_maxes[i]\n",
    "\n",
    "        mask = dataset[:, :, -1] >= 0.5 if self.use_mask else None\n",
    "\n",
    "        if not is_real_data and zero_mask_particles and self.use_mask:\n",
    "            dataset[~mask] = 0\n",
    "\n",
    "        if not is_real_data and zero_neg_pt:\n",
    "            dataset[:, :, 2][dataset[:, :, 2] < 0] = 0\n",
    "\n",
    "        return dataset[:, :, : self._num_non_mask_features], mask if ret_mask_separate else dataset\n",
    "\n",
    "    def add_noise_padding(self, dataset: Tensor):\n",
    "        \"\"\"Add Gaussian noise to zero-masked particles\"\"\"\n",
    "        # up to 5 sigmas will be within Â±1\n",
    "        noise_padding = torch.randn((len(dataset), self.num_particles, dataset.shape[2] - 1)) / 5\n",
    "        noise_padding[noise_padding > 1] = 1\n",
    "        noise_padding[noise_padding < -1] = -1\n",
    "        noise_padding[:, :, 2] /= 2.0  # pt is scaled between Â±0.5\n",
    "\n",
    "        mask = (dataset[:, :, -1] + 0.5).bool()\n",
    "        noise_padding[mask] = 0  # only adding noise to zero-masked particles\n",
    "        dataset += torch.cat(\n",
    "            (noise_padding, torch.zeros((len(dataset), self.num_particles, 1))), dim=2\n",
    "        )\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.jet_features[idx] if self.use_jet_features else self.data[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0fe7b544b91fdae879a8427963a6056d0fd4faa9d244d4855e28ddfc76a93b1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
